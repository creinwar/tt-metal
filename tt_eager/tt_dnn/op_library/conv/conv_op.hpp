// SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.
//
// SPDX-License-Identifier: Apache-2.0

#pragma once

#include "tensor/tensor.hpp"
#include "tt_dnn/op_library/run_operation.hpp"

namespace tt {

namespace tt_metal {

// TODO: Accept parallelization
enum class ConvOpParallelizationStrategy { MULTI_CORE, MULTI_CORE_REUSE, MULTI_CORE_REUSE_MCAST, SINGLE_CORE };

struct Conv {
    // additional parameters
    const std::vector<int> conv_params;
    const uint32_t act_block_h_ntiles, act_block_w_ntiles, weight_block_w_ntiles, out_subblock_h_ntiles,
        out_subblock_w_ntiles, output_channels;
    bool use_address_map, use_fast_reader, untilize_out, has_bias, fuse_relu;
    MathFidelity math_fidelity;
    Conv(
        uint32_t act_bh,
        uint32_t act_bw,
        uint32_t weight_bw,
        uint32_t out_sh,
        uint32_t out_sw,
        const std::vector<int>& c_params,
        uint32_t output_channels,
        bool address_map,
        bool fast_reader,
        bool untile_out,
        bool has_bias,
        bool fuse_relu,
        MathFidelity mfidelity) :
        act_block_h_ntiles(act_bh),
        act_block_w_ntiles(act_bw),
        weight_block_w_ntiles(weight_bw),
        out_subblock_h_ntiles(out_sh),
        out_subblock_w_ntiles(out_sw),
        output_channels(output_channels),
        conv_params(c_params),
        use_address_map(address_map),
        use_fast_reader(fast_reader),
        untilize_out(untile_out),
        has_bias(has_bias),
        fuse_relu(fuse_relu),
        math_fidelity(mfidelity) {}

    void validate(
        const std::vector<Tensor>& input_tensors,
        const std::vector<std::optional<const Tensor>>& optional_input_tensors) const;
    std::vector<Shape> compute_output_shapes(const std::vector<Tensor>& input_tensors) const;
    std::vector<Tensor> create_output_tensors(const std::vector<Tensor>& input_tensors) const;
    operation::ProgramWithCallbacks create_program(
        const std::vector<Tensor>& input_tensors,
        const std::vector<std::optional<const Tensor>>& optional_input_tensors,
        std::vector<Tensor>& output_tensors) const;

    static constexpr auto attribute_names = std::forward_as_tuple(
        "conv_params",
        "act_block_h_ntiles",
        "act_block_w_ntiles",
        "weight_block_w_ntiles",
        "out_subblock_h_ntiles",
        "out_subblock_w_ntiles",
        "output_channels",
        "use_address_map",
        "use_fast_reader",
        "untilize_out",
        "has_bias",
        "fuse_relu",
        "math_fidelity");
    const auto attribute_values() const {
        return std::forward_as_tuple(
            this->conv_params,
            this->act_block_h_ntiles,
            this->act_block_w_ntiles,
            this->weight_block_w_ntiles,
            this->out_subblock_h_ntiles,
            this->out_subblock_w_ntiles,
            this->output_channels,
            this->use_address_map,
            this->use_fast_reader,
            this->untilize_out,
            this->has_bias,
            this->fuse_relu,
            this->math_fidelity);
    }
};

Tensor conv(
    const Tensor& a,
    const Tensor& b,
    std::optional<const Tensor> bias,
    const vector<int> conv_params,
    uint32_t act_block_h_ntiles,
    uint32_t act_block_w_ntiles,
    uint32_t weight_block_w_ntiles,
    uint32_t out_subblock_h_ntiles,
    uint32_t out_subblock_w_ntiles,
    uint32_t output_channels,
    bool has_bias);

Tensor conv_with_fast_reader(
    const Tensor& a,
    const Tensor& b,
    std::optional<const Tensor> bias,
    const vector<int> conv_params,
    uint32_t act_block_h_ntiles,
    uint32_t act_block_w_ntiles,
    uint32_t weight_block_w_ntiles,
    uint32_t out_subblock_h_ntiles,
    uint32_t out_subblock_w_ntiles,
    uint32_t output_channels,
    bool untilize_out,
    bool has_bias,
    bool fuse_relu,
    MathFidelity math_fidelity = MathFidelity::HiFi4);

operation::ProgramWithCallbacks conv_single_core(
    const Tensor& A,
    const Tensor& B,
    std::optional<const Tensor> bias,
    vector<int> conv_params,
    uint32_t act_block_h_ntiles,
    uint32_t act_block_w_ntiles,
    uint32_t weight_block_w_ntiles,
    uint32_t out_subblock_h_ntiles,
    uint32_t out_subblock_w_ntiles,
    uint32_t output_channels,
    bool has_bias,
    MathFidelity math_fidelity,
    Tensor& output);  // Tilizes a, untilizes b

Tensor conv_with_address_map(
    const Tensor& a,
    const Tensor& b,
    std::optional<const Tensor> bias,
    const vector<int> conv_params,
    uint32_t act_block_h_ntiles,
    uint32_t act_block_w_ntiles,
    uint32_t weight_block_w_ntiles,
    uint32_t out_subblock_h_ntiles,
    uint32_t out_subblock_w_ntiles,
    uint32_t output_channels);
operation::ProgramWithCallbacks conv_with_address_map_single_core(
    const Tensor& A,
    const Tensor& B,
    vector<int> conv_params,
    uint32_t act_block_h_ntiles,
    uint32_t act_block_w_ntiles,
    uint32_t weight_block_w_ntiles,
    uint32_t out_subblock_h_ntiles,
    uint32_t out_subblock_w_ntiles,
    uint32_t output_channels,
    Tensor& output);  // Tilizes a, untilizes b

}  // namespace tt_metal

}  // namespace tt

// TODO: Merge with optimized_conv_op_utils?
namespace conv_op_utils {
using namespace tt;
using namespace tt::tt_metal;

pair<uint32_t, uint32_t> compute_conv_output_face_shape(
    uint32_t conv_activation_h,
    uint32_t conv_activation_w,
    uint32_t filter_h,
    uint32_t filter_w,
    uint32_t stride_h,
    uint32_t stride_w,
    uint32_t pad_h,
    uint32_t pad_w);
pair<vector<uint32_t>, vector<uint32_t>> compute_conv_activation_as_mm_shape(
    Shape conv_activation_shape,
    vector<int> conv_params,
    uint32_t act_block_h_ntiles,
    uint32_t act_block_w_ntiles,
    bool use_fast_reader);

}  // namespace conv_op_utils
